2025-12-10T16:26:47.074966Z  WARN wgpu_hal::vulkan::instance: InstanceFlags::VALIDATION requested, but unable to find layer: VK_LAYER_KHRONOS_validation
2025-12-10T16:26:47.076010Z  WARN wgpu_hal::vulkan::instance: GENERAL [Loader Message (0x0)]
	windows_read_data_files_in_registry: Registry lookup failed to get layer manifest files.
2025-12-10T16:26:47.076051Z  WARN wgpu_hal::vulkan::instance: 	objects: (type: INSTANCE, hndl: 0x204ab683090, name: ?)
2025-12-10T16:26:47.366066Z  INFO cubecl_wgpu::runtime: Using adapter AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T16:26:49.296549Z  INFO cubecl_wgpu::runtime: Created wgpu compute server on device Device { inner: Core(CoreDevice { context: ContextWgpuCore { type: "Native" }, id: Id(0,1), error_sink: Mutex { data: ErrorSink }, features: Features { features_wgpu: FeaturesWGPU(SHADER_FLOAT32_ATOMIC | TEXTURE_FORMAT_16BIT_NORM | TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES | PIPELINE_STATISTICS_QUERY | TIMESTAMP_QUERY_INSIDE_ENCODERS | TIMESTAMP_QUERY_INSIDE_PASSES | TEXTURE_BINDING_ARRAY | BUFFER_BINDING_ARRAY | STORAGE_RESOURCE_BINDING_ARRAY | SAMPLED_TEXTURE_AND_STORAGE_BUFFER_ARRAY_NON_UNIFORM_INDEXING | STORAGE_TEXTURE_ARRAY_NON_UNIFORM_INDEXING | PARTIALLY_BOUND_BINDING_ARRAY | MULTI_DRAW_INDIRECT | MULTI_DRAW_INDIRECT_COUNT | PUSH_CONSTANTS | ADDRESS_MODE_CLAMP_TO_ZERO | ADDRESS_MODE_CLAMP_TO_BORDER | POLYGON_MODE_LINE | POLYGON_MODE_POINT | CONSERVATIVE_RASTERIZATION | VERTEX_WRITABLE_STORAGE | CLEAR_TEXTURE | SPIRV_SHADER_PASSTHROUGH | MULTIVIEW | TEXTURE_ATOMIC | TEXTURE_FORMAT_NV12 | EXPERIMENTAL_RAY_TRACING_ACCELERATION_STRUCTURE | EXPERIMENTAL_RAY_QUERY | SHADER_F64 | SHADER_I16 | SHADER_PRIMITIVE_INDEX | SHADER_EARLY_DEPTH_TEST | SHADER_INT64 | SUBGROUP | SUBGROUP_VERTEX | SUBGROUP_BARRIER | PIPELINE_CACHE | SHADER_INT64_ATOMIC_MIN_MAX | SHADER_INT64_ATOMIC_ALL_OPS | VULKAN_EXTERNAL_MEMORY_WIN32 | TEXTURE_INT64_ATOMIC | EXPERIMENTAL_MESH_SHADER | EXPERIMENTAL_RAY_HIT_VERTEX_RETURN | EXPERIMENTAL_MESH_SHADER_MULTIVIEW | EXTENDED_ACCELERATION_STRUCTURE_VERTEX_FORMATS), features_webgpu: FeaturesWebGPU(DEPTH_CLIP_CONTROL | DEPTH32FLOAT_STENCIL8 | TEXTURE_COMPRESSION_BC | TEXTURE_COMPRESSION_BC_SLICED_3D | TIMESTAMP_QUERY | INDIRECT_FIRST_INSTANCE | RG11B10UFLOAT_RENDERABLE | BGRA8UNORM_STORAGE | FLOAT32_FILTERABLE | DUAL_SOURCE_BLENDING | CLIP_DISTANCES) } }) } => AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T16:26:46.979329Z  INFO burn_train::learner::train_val: Fitting the model:
 IconClassifier {
  conv1_1: Conv2d {ch_in: 3, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 896}
  conv1_2: Conv2d {ch_in: 32, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 9248}
  pool1: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv2_1: Conv2d {ch_in: 32, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 18496}
  conv2_2: Conv2d {ch_in: 64, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 36928}
  pool2: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv3_1: Conv2d {ch_in: 64, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 73856}
  conv3_2: Conv2d {ch_in: 128, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 147584}
  pool3: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  dropout1: Dropout {prob: 0.5}
  fc1: Linear {d_input: 4608, d_output: 256, bias: true, params: 1179904}
  dropout2: Dropout {prob: 0.3}
  fc2: Linear {d_input: 256, d_output: 14, bias: true, params: 3598}
  activation: Relu
  params: 1470510
}
2025-12-10T16:26:49.362712Z  INFO burn_train::learner::strategies::single::epoch: Executing training step for epoch 1
2025-12-10T16:26:49.586071Z  INFO burn_train::learner::strategies::single::epoch: Iteration 1
2025-12-10T16:26:49.590551Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:26:49.590809Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:26:49.590874Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 4, out_channels: 32, shape: [64, 64], batch_size: 32, has_bias: true, dtype: F32 }
2025-12-10T16:26:49.712728Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:26:49.712963Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:26:49.713028Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 131072, n: 32, k: 64, lhs_pow2_factor: 2, rhs_pow2_factor: 2, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:26:57.688824Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 32, shape: [64, 64], batch_size: 32, has_bias: true, dtype: F32 }
2025-12-10T16:26:57.714879Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 131072, n: 32, k: 512, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:05.349209Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 64, shape: [32, 32], batch_size: 32, has_bias: true, dtype: F32 }
2025-12-10T16:27:05.362236Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32768, n: 64, k: 512, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:12.785720Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 64, out_channels: 64, shape: [32, 32], batch_size: 32, has_bias: true, dtype: F32 }
2025-12-10T16:27:12.809679Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32768, n: 64, k: 1024, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:14.019182Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 64, out_channels: 128, shape: [16, 16], batch_size: 32, has_bias: true, dtype: F32 }
2025-12-10T16:27:14.032115Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 128, k: 1024, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:15.170762Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 128, out_channels: 128, shape: [16, 16], batch_size: 32, has_bias: true, dtype: F32 }
2025-12-10T16:27:15.193006Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 128, k: 2048, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:15.435828Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:27:15.436195Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:27:15.436274Z  INFO cubecl_runtime::tune::tuner: Tuning FusedMatmulAutotuneKey - MatmulKey: MatmulAutotuneKey { definition: MatmulProblemDefinition { m: 32, n: 256, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General } }, NumOutBuffers: 2, NumOps: 4
2025-12-10T16:27:15.436955Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32, n: 256, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:22.751550Z  INFO cubecl_runtime::tune::tuner: Tuning FusedMatmulAutotuneKey - MatmulKey: MatmulAutotuneKey { definition: MatmulProblemDefinition { m: 32, n: 16, k: 256, lhs_pow2_factor: 3, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General } }, NumOutBuffers: 1, NumOps: 2
2025-12-10T16:27:22.751850Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32, n: 16, k: 256, lhs_pow2_factor: 3, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General }
2025-12-10T16:27:23.266038Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:27:23.266346Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:27:23.266393Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 2, AxisIsContiguous: true, ReduceAxisShape: 16, ReduceCount: 64
2025-12-10T16:27:23.388154Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:27:23.388445Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:27:23.388489Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 2, axis_is_contiguous: true, reduce_axis_shape: 16, reduce_count: 64 }, FuseNumReads: 2, FuseNumWrites: 2, FuseNumOps: 2
2025-12-10T16:27:23.538518Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:27:23.538786Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:27:23.538818Z  INFO cubecl_runtime::tune::tuner: Tuning SumAutotuneKey { dtype: F32, length: 32 }
2025-12-10T16:27:23.539171Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: true, ReduceAxisShape: 32, ReduceCount: 1
2025-12-10T16:27:23.798241Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 2, axis_is_contiguous: true, reduce_axis_shape: 16, reduce_count: 64 }, FuseNumReads: 1, FuseNumWrites: 1, FuseNumOps: 1
2025-12-10T16:27:23.890511Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 1, axis_is_contiguous: true, reduce_axis_shape: 16, reduce_count: 64 }, FuseNumReads: 2, FuseNumWrites: 1, FuseNumOps: 2
2025-12-10T16:27:23.898153Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 1, AxisIsContiguous: true, ReduceAxisShape: 16, ReduceCount: 64
2025-12-10T16:27:24.093630Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 4, axis_is_contiguous: false, reduce_axis_shape: 32, reduce_count: 16 }, FuseNumReads: 4, FuseNumWrites: 2, FuseNumOps: 4
2025-12-10T16:27:24.124288Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: false, ReduceAxisShape: 32, ReduceCount: 16
2025-12-10T16:27:24.341609Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32, n: 256, k: 16, lhs_pow2_factor: 1, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General }
2025-12-10T16:27:24.978644Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 256, n: 16, k: 32, lhs_pow2_factor: 3, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: MildlyPermuted { transposed: true, batch_swap: false }, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General }
2025-12-10T16:27:25.189482Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 4, axis_is_contiguous: false, reduce_axis_shape: 32, reduce_count: 256 }, FuseNumReads: 4, FuseNumWrites: 2, FuseNumOps: 4
2025-12-10T16:27:25.198186Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: false, ReduceAxisShape: 32, ReduceCount: 256
2025-12-10T16:27:25.411306Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32, n: 8192, k: 256, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:32.700964Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 256, k: 32, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: MildlyPermuted { transposed: true, batch_swap: false }, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:34.863754Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:27:34.864048Z  INFO cubecl_runtime::tune::tune_cache: Loaded 0 autotune cached entries
2025-12-10T16:27:34.864089Z  INFO cubecl_runtime::tune::tuner: Tuning ConvTranspose2dAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], padding_out: [0, 0], dilation: [1, 1], groups: 1, in_channels: 128, out_channels: 128, height: 16, width: 16, batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:27:35.314850Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 8192, k: 128, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:43.311951Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [12, 12], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 128, shape: [16, 16], batch_size: 128, has_bias: false, dtype: F32 }
2025-12-10T16:27:43.720377Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 128, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:51.836929Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 128, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:51.931275Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: true, ReduceAxisShape: 4096, ReduceCount: 256
2025-12-10T16:27:51.991992Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 8192, k: 128, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:27:59.462886Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [12, 12], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 128, shape: [16, 16], batch_size: 64, has_bias: false, dtype: F32 }
2025-12-10T16:27:59.828526Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 4096, n: 128, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:00.031341Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 128, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:00.102766Z  INFO cubecl_runtime::tune::tuner: Tuning ConvTranspose2dAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], padding_out: [0, 0], dilation: [1, 1], groups: 1, in_channels: 64, out_channels: 64, height: 32, width: 32, batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:28:00.262342Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 32768, k: 64, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:00.315461Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [24, 24], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 64, shape: [32, 32], batch_size: 64, has_bias: false, dtype: F32 }
2025-12-10T16:28:00.811313Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 64, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:02.696470Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 64, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:02.906668Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: true, ReduceAxisShape: 4096, ReduceCount: 64
2025-12-10T16:28:02.944491Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 512, n: 32768, k: 64, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:03.181855Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [24, 24], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 64, shape: [32, 32], batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:28:03.263940Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 4096, n: 64, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:04.761667Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 512, n: 64, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:05.347238Z  INFO cubecl_runtime::tune::tuner: Tuning ConvTranspose2dAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], padding_out: [0, 0], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 32, height: 64, width: 64, batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:28:05.422003Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 512, n: 131072, k: 32, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:28:05.459347Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [48, 48], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 32, shape: [64, 64], batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:28:05.985151Z ERROR burn_train::learner::application_logger: PANIC => panicked at C:\Users\svkcy\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\cubecl-runtime-0.8.1\src\client.rs:306:41:
called `Result::unwrap()` on an `Err` value: BufferTooBig(3406823424)
2025-12-10T16:28:06.028628Z ERROR burn_train::learner::application_logger: PANIC => panicked at library\core\src\panicking.rs:225:5:
panic in a function that cannot unwind
2025-12-10T16:52:03.355348Z  WARN wgpu_hal::vulkan::instance: InstanceFlags::VALIDATION requested, but unable to find layer: VK_LAYER_KHRONOS_validation
2025-12-10T16:52:03.361183Z  WARN wgpu_hal::vulkan::instance: GENERAL [Loader Message (0x0)]
	windows_read_data_files_in_registry: Registry lookup failed to get layer manifest files.
2025-12-10T16:52:03.361214Z  WARN wgpu_hal::vulkan::instance: 	objects: (type: INSTANCE, hndl: 0x20614672ef0, name: ?)
2025-12-10T16:52:03.467319Z  INFO cubecl_wgpu::runtime: Using adapter AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T16:52:03.651028Z  INFO cubecl_wgpu::runtime: Created wgpu compute server on device Device { inner: Core(CoreDevice { context: ContextWgpuCore { type: "Native" }, id: Id(0,1), error_sink: Mutex { data: ErrorSink }, features: Features { features_wgpu: FeaturesWGPU(SHADER_FLOAT32_ATOMIC | TEXTURE_FORMAT_16BIT_NORM | TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES | PIPELINE_STATISTICS_QUERY | TIMESTAMP_QUERY_INSIDE_ENCODERS | TIMESTAMP_QUERY_INSIDE_PASSES | TEXTURE_BINDING_ARRAY | BUFFER_BINDING_ARRAY | STORAGE_RESOURCE_BINDING_ARRAY | SAMPLED_TEXTURE_AND_STORAGE_BUFFER_ARRAY_NON_UNIFORM_INDEXING | STORAGE_TEXTURE_ARRAY_NON_UNIFORM_INDEXING | PARTIALLY_BOUND_BINDING_ARRAY | MULTI_DRAW_INDIRECT | MULTI_DRAW_INDIRECT_COUNT | PUSH_CONSTANTS | ADDRESS_MODE_CLAMP_TO_ZERO | ADDRESS_MODE_CLAMP_TO_BORDER | POLYGON_MODE_LINE | POLYGON_MODE_POINT | CONSERVATIVE_RASTERIZATION | VERTEX_WRITABLE_STORAGE | CLEAR_TEXTURE | SPIRV_SHADER_PASSTHROUGH | MULTIVIEW | TEXTURE_ATOMIC | TEXTURE_FORMAT_NV12 | EXPERIMENTAL_RAY_TRACING_ACCELERATION_STRUCTURE | EXPERIMENTAL_RAY_QUERY | SHADER_F64 | SHADER_I16 | SHADER_PRIMITIVE_INDEX | SHADER_EARLY_DEPTH_TEST | SHADER_INT64 | SUBGROUP | SUBGROUP_VERTEX | SUBGROUP_BARRIER | PIPELINE_CACHE | SHADER_INT64_ATOMIC_MIN_MAX | SHADER_INT64_ATOMIC_ALL_OPS | VULKAN_EXTERNAL_MEMORY_WIN32 | TEXTURE_INT64_ATOMIC | EXPERIMENTAL_MESH_SHADER | EXPERIMENTAL_RAY_HIT_VERTEX_RETURN | EXPERIMENTAL_MESH_SHADER_MULTIVIEW | EXTENDED_ACCELERATION_STRUCTURE_VERTEX_FORMATS), features_webgpu: FeaturesWebGPU(DEPTH_CLIP_CONTROL | DEPTH32FLOAT_STENCIL8 | TEXTURE_COMPRESSION_BC | TEXTURE_COMPRESSION_BC_SLICED_3D | TIMESTAMP_QUERY | INDIRECT_FIRST_INSTANCE | RG11B10UFLOAT_RENDERABLE | BGRA8UNORM_STORAGE | FLOAT32_FILTERABLE | DUAL_SOURCE_BLENDING | CLIP_DISTANCES) } }) } => AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T16:52:03.284362Z  INFO burn_train::learner::train_val: Fitting the model:
 IconClassifier {
  conv1_1: Conv2d {ch_in: 3, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 896}
  conv1_2: Conv2d {ch_in: 32, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 9248}
  pool1: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv2_1: Conv2d {ch_in: 32, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 18496}
  conv2_2: Conv2d {ch_in: 64, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 36928}
  pool2: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv3_1: Conv2d {ch_in: 64, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 73856}
  conv3_2: Conv2d {ch_in: 128, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 147584}
  pool3: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  dropout1: Dropout {prob: 0.5}
  fc1: Linear {d_input: 4608, d_output: 256, bias: true, params: 1179904}
  dropout2: Dropout {prob: 0.3}
  fc2: Linear {d_input: 256, d_output: 14, bias: true, params: 3598}
  activation: Relu
  params: 1470510
}
2025-12-10T16:52:03.666177Z  INFO burn_train::learner::strategies::single::epoch: Executing training step for epoch 1
2025-12-10T16:52:03.685985Z  INFO burn_train::learner::strategies::single::epoch: Iteration 1
2025-12-10T16:52:03.691082Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:03.691429Z  INFO cubecl_runtime::tune::tune_cache: Loaded 10 autotune cached entries
2025-12-10T16:52:03.846725Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:03.847056Z  INFO cubecl_runtime::tune::tune_cache: Loaded 25 autotune cached entries
2025-12-10T16:52:11.627894Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:11.628152Z  INFO cubecl_runtime::tune::tune_cache: Loaded 2 autotune cached entries
2025-12-10T16:52:12.762460Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:12.762743Z  INFO cubecl_runtime::tune::tune_cache: Loaded 7 autotune cached entries
2025-12-10T16:52:12.783612Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:12.783864Z  INFO cubecl_runtime::tune::tune_cache: Loaded 5 autotune cached entries
2025-12-10T16:52:12.823698Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:12.823958Z  INFO cubecl_runtime::tune::tune_cache: Loaded 1 autotune cached entries
2025-12-10T16:52:14.698184Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:52:14.698425Z  INFO cubecl_runtime::tune::tune_cache: Loaded 3 autotune cached entries
2025-12-10T16:52:29.269185Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [48, 48], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 32, shape: [64, 64], batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:52:30.066827Z ERROR burn_train::learner::application_logger: PANIC => panicked at C:\Users\svkcy\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\cubecl-runtime-0.8.1\src\client.rs:306:41:
called `Result::unwrap()` on an `Err` value: BufferTooBig(3406823424)
2025-12-10T16:56:04.294652Z  WARN wgpu_hal::vulkan::instance: InstanceFlags::VALIDATION requested, but unable to find layer: VK_LAYER_KHRONOS_validation
2025-12-10T16:56:04.295545Z  WARN wgpu_hal::vulkan::instance: GENERAL [Loader Message (0x0)]
	windows_read_data_files_in_registry: Registry lookup failed to get layer manifest files.
2025-12-10T16:56:04.295571Z  WARN wgpu_hal::vulkan::instance: 	objects: (type: INSTANCE, hndl: 0x2c9a1ea9a10, name: ?)
2025-12-10T16:56:04.342200Z  INFO cubecl_wgpu::runtime: Using adapter AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T16:56:04.475750Z  INFO cubecl_wgpu::runtime: Created wgpu compute server on device Device { inner: Core(CoreDevice { context: ContextWgpuCore { type: "Native" }, id: Id(0,1), error_sink: Mutex { data: ErrorSink }, features: Features { features_wgpu: FeaturesWGPU(SHADER_FLOAT32_ATOMIC | TEXTURE_FORMAT_16BIT_NORM | TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES | PIPELINE_STATISTICS_QUERY | TIMESTAMP_QUERY_INSIDE_ENCODERS | TIMESTAMP_QUERY_INSIDE_PASSES | TEXTURE_BINDING_ARRAY | BUFFER_BINDING_ARRAY | STORAGE_RESOURCE_BINDING_ARRAY | SAMPLED_TEXTURE_AND_STORAGE_BUFFER_ARRAY_NON_UNIFORM_INDEXING | STORAGE_TEXTURE_ARRAY_NON_UNIFORM_INDEXING | PARTIALLY_BOUND_BINDING_ARRAY | MULTI_DRAW_INDIRECT | MULTI_DRAW_INDIRECT_COUNT | PUSH_CONSTANTS | ADDRESS_MODE_CLAMP_TO_ZERO | ADDRESS_MODE_CLAMP_TO_BORDER | POLYGON_MODE_LINE | POLYGON_MODE_POINT | CONSERVATIVE_RASTERIZATION | VERTEX_WRITABLE_STORAGE | CLEAR_TEXTURE | SPIRV_SHADER_PASSTHROUGH | MULTIVIEW | TEXTURE_ATOMIC | TEXTURE_FORMAT_NV12 | EXPERIMENTAL_RAY_TRACING_ACCELERATION_STRUCTURE | EXPERIMENTAL_RAY_QUERY | SHADER_F64 | SHADER_I16 | SHADER_PRIMITIVE_INDEX | SHADER_EARLY_DEPTH_TEST | SHADER_INT64 | SUBGROUP | SUBGROUP_VERTEX | SUBGROUP_BARRIER | PIPELINE_CACHE | SHADER_INT64_ATOMIC_MIN_MAX | SHADER_INT64_ATOMIC_ALL_OPS | VULKAN_EXTERNAL_MEMORY_WIN32 | TEXTURE_INT64_ATOMIC | EXPERIMENTAL_MESH_SHADER | EXPERIMENTAL_RAY_HIT_VERTEX_RETURN | EXPERIMENTAL_MESH_SHADER_MULTIVIEW | EXTENDED_ACCELERATION_STRUCTURE_VERTEX_FORMATS), features_webgpu: FeaturesWebGPU(DEPTH_CLIP_CONTROL | DEPTH32FLOAT_STENCIL8 | TEXTURE_COMPRESSION_BC | TEXTURE_COMPRESSION_BC_SLICED_3D | TIMESTAMP_QUERY | INDIRECT_FIRST_INSTANCE | RG11B10UFLOAT_RENDERABLE | BGRA8UNORM_STORAGE | FLOAT32_FILTERABLE | DUAL_SOURCE_BLENDING | CLIP_DISTANCES) } }) } => AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T16:56:04.223648Z  INFO burn_train::learner::train_val: Fitting the model:
 IconClassifier {
  conv1_1: Conv2d {ch_in: 3, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 896}
  conv1_2: Conv2d {ch_in: 32, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 9248}
  pool1: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv2_1: Conv2d {ch_in: 32, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 18496}
  conv2_2: Conv2d {ch_in: 64, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 36928}
  pool2: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv3_1: Conv2d {ch_in: 64, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 73856}
  conv3_2: Conv2d {ch_in: 128, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 147584}
  pool3: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  dropout1: Dropout {prob: 0.5}
  fc1: Linear {d_input: 4608, d_output: 256, bias: true, params: 1179904}
  dropout2: Dropout {prob: 0.3}
  fc2: Linear {d_input: 256, d_output: 14, bias: true, params: 3598}
  activation: Relu
  params: 1470510
}
2025-12-10T16:56:04.501046Z  INFO burn_train::learner::strategies::single::epoch: Executing training step for epoch 1
2025-12-10T16:56:04.507666Z  INFO burn_train::learner::strategies::single::epoch: Iteration 1
2025-12-10T16:56:04.512395Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:04.512711Z  INFO cubecl_runtime::tune::tune_cache: Loaded 10 autotune cached entries
2025-12-10T16:56:04.512762Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 4, out_channels: 32, shape: [64, 64], batch_size: 8, has_bias: true, dtype: F32 }
2025-12-10T16:56:04.612169Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:04.612514Z  INFO cubecl_runtime::tune::tune_cache: Loaded 25 autotune cached entries
2025-12-10T16:56:04.612564Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32768, n: 32, k: 64, lhs_pow2_factor: 2, rhs_pow2_factor: 2, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:12.219670Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 32, shape: [64, 64], batch_size: 8, has_bias: true, dtype: F32 }
2025-12-10T16:56:12.267620Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 32768, n: 32, k: 512, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:18.872959Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 64, shape: [32, 32], batch_size: 8, has_bias: true, dtype: F32 }
2025-12-10T16:56:18.879906Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 64, k: 512, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:25.440612Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 64, out_channels: 64, shape: [32, 32], batch_size: 8, has_bias: true, dtype: F32 }
2025-12-10T16:56:25.476476Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 64, k: 1024, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:26.216160Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 64, out_channels: 128, shape: [16, 16], batch_size: 8, has_bias: true, dtype: F32 }
2025-12-10T16:56:26.235502Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 128, k: 1024, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:27.162058Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 128, k: 1024, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Medium, kind: General }
2025-12-10T16:56:28.755211Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 128, out_channels: 128, shape: [16, 16], batch_size: 8, has_bias: true, dtype: F32 }
2025-12-10T16:56:28.781031Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 128, k: 2048, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:29.001416Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 128, k: 2048, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Medium, kind: General }
2025-12-10T16:56:29.096294Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:29.096572Z  INFO cubecl_runtime::tune::tune_cache: Loaded 2 autotune cached entries
2025-12-10T16:56:29.096607Z  INFO cubecl_runtime::tune::tuner: Tuning FusedMatmulAutotuneKey - MatmulKey: MatmulAutotuneKey { definition: MatmulProblemDefinition { m: 8, n: 256, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General } }, NumOutBuffers: 2, NumOps: 4
2025-12-10T16:56:29.097001Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8, n: 256, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:36.551010Z  INFO cubecl_runtime::tune::tuner: Tuning FusedMatmulAutotuneKey - MatmulKey: MatmulAutotuneKey { definition: MatmulProblemDefinition { m: 8, n: 16, k: 256, lhs_pow2_factor: 3, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General } }, NumOutBuffers: 1, NumOps: 2
2025-12-10T16:56:36.551291Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8, n: 16, k: 256, lhs_pow2_factor: 3, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General }
2025-12-10T16:56:36.922753Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:36.923027Z  INFO cubecl_runtime::tune::tune_cache: Loaded 7 autotune cached entries
2025-12-10T16:56:36.923058Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 2, AxisIsContiguous: true, ReduceAxisShape: 16, ReduceCount: 16
2025-12-10T16:56:37.021299Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:37.021566Z  INFO cubecl_runtime::tune::tune_cache: Loaded 5 autotune cached entries
2025-12-10T16:56:37.021605Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 2, axis_is_contiguous: true, reduce_axis_shape: 16, reduce_count: 16 }, FuseNumReads: 2, FuseNumWrites: 2, FuseNumOps: 2
2025-12-10T16:56:37.135249Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:37.135545Z  INFO cubecl_runtime::tune::tune_cache: Loaded 1 autotune cached entries
2025-12-10T16:56:37.135575Z  INFO cubecl_runtime::tune::tuner: Tuning SumAutotuneKey { dtype: F32, length: 8 }
2025-12-10T16:56:37.135763Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: true, ReduceAxisShape: 16, ReduceCount: 1
2025-12-10T16:56:37.332099Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 2, axis_is_contiguous: true, reduce_axis_shape: 16, reduce_count: 16 }, FuseNumReads: 1, FuseNumWrites: 1, FuseNumOps: 1
2025-12-10T16:56:37.441858Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 1, axis_is_contiguous: true, reduce_axis_shape: 16, reduce_count: 16 }, FuseNumReads: 2, FuseNumWrites: 1, FuseNumOps: 2
2025-12-10T16:56:37.445515Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 1, AxisIsContiguous: true, ReduceAxisShape: 16, ReduceCount: 16
2025-12-10T16:56:37.673638Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 4, axis_is_contiguous: false, reduce_axis_shape: 16, reduce_count: 16 }, FuseNumReads: 4, FuseNumWrites: 2, FuseNumOps: 4
2025-12-10T16:56:37.678729Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: false, ReduceAxisShape: 16, ReduceCount: 16
2025-12-10T16:56:37.863476Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8, n: 256, k: 16, lhs_pow2_factor: 1, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General }
2025-12-10T16:56:38.547646Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 256, n: 16, k: 8, lhs_pow2_factor: 3, rhs_pow2_factor: 1, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: MildlyPermuted { transposed: true, batch_swap: false }, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Small, kind: General }
2025-12-10T16:56:38.764811Z  INFO cubecl_runtime::tune::tuner: Tuning FusedReduceAutotuneKey - ReduceKey: ReduceAutotuneKey { elem_input: Float(F32), elem_output: Float(F32), elem_acc: Float(F32), potential_line_size: 4, axis_is_contiguous: false, reduce_axis_shape: 16, reduce_count: 256 }, FuseNumReads: 4, FuseNumWrites: 2, FuseNumOps: 4
2025-12-10T16:56:38.768090Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: false, ReduceAxisShape: 16, ReduceCount: 256
2025-12-10T16:56:38.969753Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8, n: 8192, k: 256, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:44.712301Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 256, k: 8, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: MildlyPermuted { transposed: true, batch_swap: false }, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:46.801382Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T16:56:46.801696Z  INFO cubecl_runtime::tune::tune_cache: Loaded 3 autotune cached entries
2025-12-10T16:56:46.801724Z  INFO cubecl_runtime::tune::tuner: Tuning ConvTranspose2dAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], padding_out: [0, 0], dilation: [1, 1], groups: 1, in_channels: 128, out_channels: 128, height: 16, width: 16, batch_size: 8, has_bias: false, dtype: F32 }
2025-12-10T16:56:47.042635Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 2048, k: 128, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:56:55.208887Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 2048, k: 128, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Medium, kind: General }
2025-12-10T16:56:55.231995Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [12, 12], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 8, out_channels: 128, shape: [16, 16], batch_size: 128, has_bias: false, dtype: F32 }
2025-12-10T16:57:01.205984Z  INFO cubecl_runtime::tune::tuner: Tuning ReduceAutotuneKey - ElemInput: Float(F32), ElemOutput: Float(F32), ElemAcc: Float(F32), PotentialLineSize: 4, AxisIsContiguous: true, ReduceAxisShape: 2048, ReduceCount: 256
2025-12-10T16:57:01.252435Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 2048, k: 128, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Medium, kind: General }
2025-12-10T16:57:02.545124Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [12, 12], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 8, out_channels: 128, shape: [16, 16], batch_size: 64, has_bias: false, dtype: F32 }
2025-12-10T16:57:02.603384Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 4096, n: 128, k: 2048, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:02.884958Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 128, k: 2048, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Medium, kind: General }
2025-12-10T16:57:02.913139Z  INFO cubecl_runtime::tune::tuner: Tuning ConvTranspose2dAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], padding_out: [0, 0], dilation: [1, 1], groups: 1, in_channels: 64, out_channels: 64, height: 32, width: 32, batch_size: 8, has_bias: false, dtype: F32 }
2025-12-10T16:57:02.991005Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 8192, k: 64, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:08.451021Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [24, 24], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 8, out_channels: 64, shape: [32, 32], batch_size: 64, has_bias: false, dtype: F32 }
2025-12-10T16:57:08.708862Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 8192, n: 64, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:09.906335Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 1024, n: 64, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:09.975361Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [24, 24], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 8, out_channels: 64, shape: [32, 32], batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:57:09.999146Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 4096, n: 64, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:11.228005Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 512, n: 64, k: 8192, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:11.681296Z  INFO cubecl_runtime::tune::tuner: Tuning ConvTranspose2dAutotuneKey { kernel_size: [3, 3], stride: [1, 1], padding: [1, 1], padding_out: [0, 0], dilation: [1, 1], groups: 1, in_channels: 32, out_channels: 32, height: 64, width: 64, batch_size: 8, has_bias: false, dtype: F32 }
2025-12-10T16:57:11.706112Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 512, n: 32768, k: 32, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: Contiguous }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:11.952858Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [48, 48], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 8, out_channels: 32, shape: [64, 64], batch_size: 32, has_bias: false, dtype: F32 }
2025-12-10T16:57:12.118782Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 16384, n: 32, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:20.204236Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 512, n: 32, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:20.364998Z  INFO cubecl_runtime::tune::tuner: Tuning ConvAutotuneKey { kernel_size: [48, 48], stride: [1, 1], padding: [1, 1], dilation: [1, 1], groups: 1, in_channels: 8, out_channels: 32, shape: [64, 64], batch_size: 4, has_bias: false, dtype: F32 }
2025-12-10T16:57:20.395520Z  INFO cubecl_runtime::tune::tuner: Tuning MatmulAutotuneKey - Definition: MatmulProblemDefinition { m: 2048, n: 32, k: 32768, lhs_pow2_factor: 3, rhs_pow2_factor: 3, elem_lhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_rhs: MatmulElemType { elem: Float(F32), quantized: false }, elem_out: MatmulElemType { elem: Float(F32), quantized: false }, matrix_layout_lhs: Contiguous, matrix_layout_rhs: MildlyPermuted { transposed: true, batch_swap: false } }, Analysis: MatmulAutotuneAnalysis { scale_global: Large, kind: General }
2025-12-10T16:57:28.259213Z  INFO burn_train::learner::strategies::single::epoch: Iteration 2
2025-12-10T16:57:28.416107Z  INFO burn_train::learner::strategies::single::epoch: Iteration 3
2025-12-10T16:57:28.449821Z  INFO burn_train::learner::strategies::single::epoch: Iteration 4
2025-12-10T16:57:28.515031Z  INFO burn_train::learner::strategies::single::epoch: Iteration 5
2025-12-10T16:57:28.609434Z  INFO burn_train::learner::strategies::single::epoch: Iteration 6
2025-12-10T16:57:28.668950Z  INFO burn_train::learner::strategies::single::epoch: Iteration 7
2025-12-10T16:57:28.726376Z  INFO burn_train::learner::strategies::single::epoch: Iteration 8
2025-12-10T16:57:28.784492Z  INFO burn_train::learner::strategies::single::epoch: Iteration 9
2025-12-10T16:57:28.840993Z  INFO burn_train::learner::strategies::single::epoch: Iteration 10
2025-12-10T16:57:28.899549Z  INFO burn_train::learner::strategies::single::epoch: Iteration 11
2025-12-10T16:57:28.981987Z  INFO burn_train::learner::strategies::single::epoch: Iteration 12
2025-12-10T16:57:29.041551Z  INFO burn_train::learner::strategies::single::epoch: Iteration 13
2025-12-10T16:57:29.099998Z  INFO burn_train::learner::strategies::single::epoch: Iteration 14
2025-12-10T16:57:29.158950Z  INFO burn_train::learner::strategies::single::epoch: Iteration 15
2025-12-10T16:57:29.224161Z  INFO burn_train::learner::strategies::single::epoch: Iteration 16
2025-12-10T16:57:29.283209Z  INFO burn_train::learner::strategies::single::epoch: Iteration 17
2025-12-10T16:57:29.345391Z  INFO burn_train::learner::strategies::single::epoch: Iteration 18
2025-12-10T16:57:29.404264Z  INFO burn_train::learner::strategies::single::epoch: Iteration 19
2025-12-10T17:01:41.680048Z  WARN wgpu_hal::vulkan::instance: InstanceFlags::VALIDATION requested, but unable to find layer: VK_LAYER_KHRONOS_validation
2025-12-10T17:01:41.681063Z  WARN wgpu_hal::vulkan::instance: GENERAL [Loader Message (0x0)]
	windows_read_data_files_in_registry: Registry lookup failed to get layer manifest files.
2025-12-10T17:01:41.681085Z  WARN wgpu_hal::vulkan::instance: 	objects: (type: INSTANCE, hndl: 0x167cd5e1d70, name: ?)
2025-12-10T17:01:41.728352Z  INFO cubecl_wgpu::runtime: Using adapter AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T17:01:41.860059Z  INFO cubecl_wgpu::runtime: Created wgpu compute server on device Device { inner: Core(CoreDevice { context: ContextWgpuCore { type: "Native" }, id: Id(0,1), error_sink: Mutex { data: ErrorSink }, features: Features { features_wgpu: FeaturesWGPU(SHADER_FLOAT32_ATOMIC | TEXTURE_FORMAT_16BIT_NORM | TEXTURE_ADAPTER_SPECIFIC_FORMAT_FEATURES | PIPELINE_STATISTICS_QUERY | TIMESTAMP_QUERY_INSIDE_ENCODERS | TIMESTAMP_QUERY_INSIDE_PASSES | TEXTURE_BINDING_ARRAY | BUFFER_BINDING_ARRAY | STORAGE_RESOURCE_BINDING_ARRAY | SAMPLED_TEXTURE_AND_STORAGE_BUFFER_ARRAY_NON_UNIFORM_INDEXING | STORAGE_TEXTURE_ARRAY_NON_UNIFORM_INDEXING | PARTIALLY_BOUND_BINDING_ARRAY | MULTI_DRAW_INDIRECT | MULTI_DRAW_INDIRECT_COUNT | PUSH_CONSTANTS | ADDRESS_MODE_CLAMP_TO_ZERO | ADDRESS_MODE_CLAMP_TO_BORDER | POLYGON_MODE_LINE | POLYGON_MODE_POINT | CONSERVATIVE_RASTERIZATION | VERTEX_WRITABLE_STORAGE | CLEAR_TEXTURE | SPIRV_SHADER_PASSTHROUGH | MULTIVIEW | TEXTURE_ATOMIC | TEXTURE_FORMAT_NV12 | EXPERIMENTAL_RAY_TRACING_ACCELERATION_STRUCTURE | EXPERIMENTAL_RAY_QUERY | SHADER_F64 | SHADER_I16 | SHADER_PRIMITIVE_INDEX | SHADER_EARLY_DEPTH_TEST | SHADER_INT64 | SUBGROUP | SUBGROUP_VERTEX | SUBGROUP_BARRIER | PIPELINE_CACHE | SHADER_INT64_ATOMIC_MIN_MAX | SHADER_INT64_ATOMIC_ALL_OPS | VULKAN_EXTERNAL_MEMORY_WIN32 | TEXTURE_INT64_ATOMIC | EXPERIMENTAL_MESH_SHADER | EXPERIMENTAL_RAY_HIT_VERTEX_RETURN | EXPERIMENTAL_MESH_SHADER_MULTIVIEW | EXTENDED_ACCELERATION_STRUCTURE_VERTEX_FORMATS), features_webgpu: FeaturesWebGPU(DEPTH_CLIP_CONTROL | DEPTH32FLOAT_STENCIL8 | TEXTURE_COMPRESSION_BC | TEXTURE_COMPRESSION_BC_SLICED_3D | TIMESTAMP_QUERY | INDIRECT_FIRST_INSTANCE | RG11B10UFLOAT_RENDERABLE | BGRA8UNORM_STORAGE | FLOAT32_FILTERABLE | DUAL_SOURCE_BLENDING | CLIP_DISTANCES) } }) } => AdapterInfo { name: "NVIDIA GeForce RTX 3090 Ti", vendor: 4318, device: 8707, device_type: DiscreteGpu, driver: "NVIDIA", driver_info: "581.57", backend: Vulkan }
2025-12-10T17:01:41.610232Z  INFO burn_train::learner::train_val: Fitting the model:
 IconClassifier {
  conv1_1: Conv2d {ch_in: 3, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 896}
  conv1_2: Conv2d {ch_in: 32, ch_out: 32, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 9248}
  pool1: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv2_1: Conv2d {ch_in: 32, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 18496}
  conv2_2: Conv2d {ch_in: 64, ch_out: 64, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 36928}
  pool2: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  conv3_1: Conv2d {ch_in: 64, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 73856}
  conv3_2: Conv2d {ch_in: 128, ch_out: 128, stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Explicit(1, 1), params: 147584}
  pool3: MaxPool2d {kernel_size: [2, 2], stride: [2, 2], padding: Valid, dilation: [1, 1]}
  dropout1: Dropout {prob: 0.5}
  fc1: Linear {d_input: 4608, d_output: 256, bias: true, params: 1179904}
  dropout2: Dropout {prob: 0.3}
  fc2: Linear {d_input: 256, d_output: 14, bias: true, params: 3598}
  activation: Relu
  params: 1470510
}
2025-12-10T17:01:41.875661Z  INFO burn_train::learner::strategies::single::epoch: Executing training step for epoch 1
2025-12-10T17:01:41.953788Z  INFO burn_train::learner::strategies::single::epoch: Iteration 1
2025-12-10T17:01:41.959890Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:41.960207Z  INFO cubecl_runtime::tune::tune_cache: Loaded 22 autotune cached entries
2025-12-10T17:01:42.085257Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:42.085604Z  INFO cubecl_runtime::tune::tune_cache: Loaded 53 autotune cached entries
2025-12-10T17:01:45.048970Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:45.049249Z  INFO cubecl_runtime::tune::tune_cache: Loaded 4 autotune cached entries
2025-12-10T17:01:46.148476Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:46.148752Z  INFO cubecl_runtime::tune::tune_cache: Loaded 13 autotune cached entries
2025-12-10T17:01:46.154224Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:46.154479Z  INFO cubecl_runtime::tune::tune_cache: Loaded 10 autotune cached entries
2025-12-10T17:01:46.185907Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:46.186255Z  INFO cubecl_runtime::tune::tune_cache: Loaded 2 autotune cached entries
2025-12-10T17:01:47.008991Z  INFO cubecl_runtime::tune::tune_cache: Load autotune cache ...
2025-12-10T17:01:47.009256Z  INFO cubecl_runtime::tune::tune_cache: Loaded 6 autotune cached entries
2025-12-10T17:01:56.783015Z  INFO burn_train::learner::strategies::single::epoch: Iteration 2
2025-12-10T17:01:56.901842Z  INFO burn_train::learner::strategies::single::epoch: Iteration 3
2025-12-10T17:01:56.971518Z  INFO burn_train::learner::strategies::single::epoch: Iteration 4
2025-12-10T17:01:57.040369Z  INFO burn_train::learner::strategies::single::epoch: Iteration 5
2025-12-10T17:01:57.101629Z  INFO burn_train::learner::strategies::single::epoch: Iteration 6
2025-12-10T17:01:57.191064Z  INFO burn_train::learner::strategies::single::epoch: Iteration 7
2025-12-10T17:01:57.255585Z  INFO burn_train::learner::strategies::single::epoch: Iteration 8
2025-12-10T17:01:57.316019Z  INFO burn_train::learner::strategies::single::epoch: Iteration 9
2025-12-10T17:01:57.376314Z  INFO burn_train::learner::strategies::single::epoch: Iteration 10
2025-12-10T17:01:57.434727Z  INFO burn_train::learner::strategies::single::epoch: Iteration 11
2025-12-10T17:01:57.490941Z  INFO burn_train::learner::strategies::single::epoch: Iteration 12
2025-12-10T17:01:57.597279Z  INFO burn_train::learner::strategies::single::epoch: Iteration 13
2025-12-10T17:01:57.665499Z  INFO burn_train::learner::strategies::single::epoch: Iteration 14
2025-12-10T17:01:57.726397Z  INFO burn_train::learner::strategies::single::epoch: Iteration 15
